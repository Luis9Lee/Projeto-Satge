# Documenta√ß√£o do Apache Airflow - Pipeline Databricks

## Vis√£o Geral da Orquestra√ß√£o

O Apache Airflow √© utilizado para orquestrar todo o pipeline de dados no Databricks, garantindo execu√ß√£o agendada, monitoramento e controle de depend√™ncias entre as camadas Bronze, Silver e Gold.

## Estrutura da DAG

### Configura√ß√µes Principais

```python
"""
DAG: etl_databricks_principal
Descri√ß√£o: Orquestra o pipeline completo de dados COVID-19 e economia no Databricks
Arquitetura: Bronze ‚Üí Silver ‚Üí Gold ‚Üí Insights (Medallion Architecture)
Respons√°vel: Engenharia de Dados
"""

from airflow import DAG
from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator
from airflow.utils.dates import days_ago
import pendulum

# Configura√ß√µes da DAG
default_args = {
    'owner': 'engenharia_dados',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': pendulum.duration(minutes=5)
}

with DAG(
    dag_id='etl_databricks_principal',
    default_args=default_args,
    description='Pipeline completo ETL COVID-19 e dados econ√¥micos no Databricks',
    schedule_interval='0 2 * * *',
    start_date=pendulum.datetime(2025, 1, 1, tz='America/Sao_Paulo'),
    end_date=None,
    catchup=False,
    tags=['etl', 'databricks', 'producao', 'covid', 'economia'],
    max_active_runs=1,
    concurrency=1
) as dag:

    # Tarefa 1: Camada Bronze - Ingest√£o de dados brutos
    bronze_covid_19 = DatabricksRunNowOperator(
        task_id='executar_bronze_covid_19',
        databricks_conn_id='databricks_default',
        notebook_params={
            'data_processamento': '{{ ds }}',
            'ambiente': 'producao',
            'camada': 'bronze',
            'reprocessar': 'false',
            'executar_api_worldbank': 'true'
        },
        python_params=None,
        spark_submit_params=None
    )
    bronze_covid_19.notebook_path = "/Workspace/Users/luisgaltm@gmail.com/desafio stage/Bronze_covid-19"

    # Tarefa 2: Camada Silver - Processamento e qualidade
    silver_covid_19 = DatabricksRunNowOperator(
        task_id='executar_silver_covid_19',
        databricks_conn_id='databricks_default',
        notebook_params={
            'data_processamento': '{{ ds }}',
            'ambiente': 'producao', 
            'camada': 'silver',
            'reprocessar': 'false',
            'validar_qualidade': 'true',
            'aplicar_limpeza': 'true',
            'executar_enriquecimento': 'true'
        },
        python_params=None,
        spark_submit_params=None
    )
    silver_covid_19.notebook_path = "/Workspace/Users/luisgaltm@gmail.com/desafio stage/Silver_covid-19"

    # Tarefa 3: Camada Gold - Modelagem anal√≠tica
    gold_covid_19 = DatabricksRunNowOperator(
        task_id='executar_gold_covid_19',
        databricks_conn_id='databricks_default',
        notebook_params={
            'data_processamento': '{{ ds }}',
            'ambiente': 'producao',
            'camada': 'gold', 
            'gerar_agregados_anuais': 'true',
            'calcular_metricas_continente': 'true',
            'criar_rankings_paises': 'true'
        },
        python_params=None,
        spark_submit_params=None
    )
    gold_covid_19.notebook_path = "/Workspace/Users/luisgaltm@gmail.com/desafio stage/Gold_covid-19"

    # Tarefa 4: Insights e relat√≥rios executivos
    insights_gold_covid_19 = DatabricksRunNowOperator(
        task_id='executar_insights_gold_covid_19',
        databricks_conn_id='databricks_default',
        notebook_params={
            'data_processamento': '{{ ds }}',
            'ambiente': 'producao',
            'gerar_insights': 'true',
            'executar_consultas_analiticas': 'true',
            'criar_relatorio_executivo': 'true',
            'enviar_notificacao': 'true'
        },
        python_params=None,
        spark_submit_params=None
    )
    insights_gold_covid_19.notebook_path = "/Workspace/Users/luisgaltm@gmail.com/desafio stage/Insights_Da_Gold_covid-19"

    # Definir depend√™ncias entre as tarefas
    bronze_covid_19 >> silver_covid_19 >> gold_covid_19 >> insights_gold_covid_19

    # Documenta√ß√£o das tarefas
    bronze_covid_19.doc_md = """
    ### Bronze_covid-19 - Ingest√£o de Dados Brutos
    
    **Caminho:** `/Workspace/Users/luisgaltm@gmail.com/desafio stage/Bronze_covid-19`
    
    **Objetivo:** Ingest√£o de todas as fontes de dados COVID-19 e econ√¥micos
    **Fontes Processadas:**
    - country_dataset.csv (delimitador ;)
    - cases_dataset.csv
    - hospital_dataset.csv
    - tests_dataset.csv
    - vaccination_dataset.csv
    - API World Bank (GDP growth)
    
    **Sa√≠da:** Tabelas no schema bronze do Unity Catalog
    **Tabelas Criadas:** country_raw, cases_raw, hospital_raw, tests_raw, vaccination_raw, gdp_raw
    """

    silver_covid_19.doc_md = """
    ### Silver_covid-19 - Processamento e Qualidade
    
    **Caminho:** `/Workspace/Users/luisgaltm@gmail.com/desafio stage/Silver_covid-19`
    
    **Objetivo:** Limpeza, valida√ß√£o e enriquecimento dos dados brutos
    **Processos Executados:**
    - Convers√£o segura de tipos num√©ricos
    - Tratamento de valores missing e outliers
    - Valida√ß√£o de consist√™ncia temporal
    - Enriquecimento com joins estrat√©gicos
    - C√°lculo de m√©tricas derivadas (casos/milh√£o, etc.)
    
    **Sa√≠da:** Tabelas no schema silver do Unity Catalog  
    **Tabelas Criadas:** country_cleaned, cases_cleaned, vaccination_cleaned, gdp_cleaned, covid_enriched
    """

    gold_covid_19.doc_md = """
    ### Gold_covid-19 - Modelagem Anal√≠tica
    
    **Caminho:** `/Workspace/Users/luisgaltm@gmail.com/desafio stage/Gold_covid-19`
    
    **Objetivo:** Cria√ß√£o de modelos anal√≠ticos e agregados estrat√©gicos
    **Processos Executados:**
    - Agrega√ß√µes anuais por pa√≠s e continente
    - C√°lculo de correla√ß√µes COVID-economia
    - Cria√ß√£o de rankings de pa√≠ses
    - Desenvolvimento de √≠ndices de resili√™ncia
    - Prepara√ß√£o para dashboards executivos
    
    **Sa√≠da:** Tabelas no schema gold do Unity Catalog
    **Tabelas Criadas:** covid_economic_annual, continent_metrics, top_countries_economic_impact, top_countries_best_response
    """

    insights_gold_covid_19.doc_md = """
    ### Insights_Da_Gold_covid-19 - An√°lise e Relat√≥rios
    
    **Caminho:** `/Workspace/Users/luisgaltm@gmail.com/desafio stage/Insights_Da_Gold_covid-19`
    
    **Objetivo:** Gera√ß√£o de insights estrat√©gicos e relat√≥rios executivos
    **Processos Executados:**
    - An√°lise de correla√ß√µes entre vari√°veis
    - Identifica√ß√£o de padr√µes e tend√™ncias
    - Gera√ß√£o de relat√≥rios executivos
    - Prepara√ß√£o de dados para apresenta√ß√£o
    - C√°lculo de m√©tricas de neg√≥cio
    
    **Sa√≠das:**
    - Relat√≥rios PDF para diretoria
    - Insights para tomada de decis√£o
    - M√©tricas de performance do pipeline
    - Alertas e notifica√ß√µes
    """
```

Esta DAG implementa uma orquestra√ß√£o robusta e escal√°vel para o pipeline de dados, proporcionando:

- **Agendamento Confi√°vel**: Execu√ß√£o di√°ria autom√°tica
- **Controle de Depend√™ncias**: Garantia da ordem correta de processamento
- **Monitoramento**: Alertas e m√©tricas de performance
- **Manutenibilidade**: Documenta√ß√£o completa e par√¢metros configur√°veis
- **Escalabilidade**: Facilidade de adicionar novas fontes ou transforma√ß√µes

O pipeline est√° pronto para opera√ß√£o em ambiente de produ√ß√£o, fornecendo dados confi√°veis e insights estrat√©gicos para an√°lise do impacto da COVID-19 na economia global.

<img width="899" height="153" alt="image" src="https://github.com/user-attachments/assets/dbcab3dc-278a-4256-a539-19ae5fbe767f" />

# Documenta√ß√£o do Apache Airflow - Pipeline Databricks e Google Colab

## Vis√£o Geral da Orquestra√ß√£o
O Apache Airflow √© utilizado para orquestrar todo o pipeline de dados tanto no Databricks quanto no Google Colab, garantindo execu√ß√£o agendada, monitoramento e controle de depend√™ncias entre as camadas Bronze, Silver e Gold.

---

## Estrutura da DAG Principal - Databricks

```python
"""
DAG: etl_databricks_principal
Descri√ß√£o: Orquestra o pipeline completo de dados COVID-19 e economia no Databricks
Arquitetura: Bronze ‚Üí Silver ‚Üí Gold ‚Üí Insights (Medallion Architecture)
Respons√°vel: Engenharia de Dados
"""

from airflow import DAG
from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator
from airflow.utils.dates import days_ago
import pendulum

# Configura√ß√µes da DAG
default_args = {
    'owner': 'engenharia_dados',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': pendulum.duration(minutes=5)
}

with DAG(
    dag_id='etl_databricks_principal',
    default_args=default_args,
    description='Pipeline completo ETL COVID-19 e dados econ√¥micos no Databricks',
    schedule_interval='0 2 * * *',
    start_date=pendulum.datetime(2025, 1, 1, tz='America/Sao_Paulo'),
    end_date=None,
    catchup=False,
    tags=['etl', 'databricks', 'producao', 'covid', 'economia'],
    max_active_runs=1,
    concurrency=1
) as dag:

    # Tarefa 1: Camada Bronze - Ingest√£o de dados brutos
    bronze_covid_19 = DatabricksRunNowOperator(
        task_id='executar_bronze_covid_19',
        databricks_conn_id='databricks_default',
        notebook_params={
            'data_processamento': '{{ ds }}',
            'ambiente': 'producao',
            'camada': 'bronze',
            'reprocessar': 'false',
            'executar_api_worldbank': 'true'
        },
        python_params=None,
        spark_submit_params=None
    )
    bronze_covid_19.notebook_path = "/Workspace/Users/luisgaltm@gmail.com/desafio stage/Bronze_covid-19"

    # Tarefa 2: Camada Silver - Processamento e qualidade
    silver_covid_19 = DatabricksRunNowOperator(
        task_id='executar_silver_covid_19',
        databricks_conn_id='databricks_default',
        notebook_params={
            'data_processamento': '{{ ds }}',
            'ambiente': 'producao', 
            'camada': 'silver',
            'reprocessar': 'false',
            'validar_qualidade': 'true',
            'aplicar_limpeza': 'true',
            'executar_enriquecimento': 'true'
        },
        python_params=None,
        spark_submit_params=None
    )
    silver_covid_19.notebook_path = "/Workspace/Users/luisgaltm@gmail.com/desafio stage/Silver_covid-19"

    # Tarefa 3: Camada Gold - Modelagem anal√≠tica
    gold_covid_19 = DatabricksRunNowOperator(
        task_id='executar_gold_covid_19',
        databricks_conn_id='databricks_default',
        notebook_params={
            'data_processamento': '{{ ds }}',
            'ambiente': 'producao',
            'camada': 'gold', 
            'gerar_agregados_anuais': 'true',
            'calcular_metricas_continente': 'true',
            'criar_rankings_paises': 'true'
        },
        python_params=None,
        spark_submit_params=None
    )
    gold_covid_19.notebook_path = "/Workspace/Users/luisgaltm@gmail.com/desafio stage/Gold_covid-19"

    # Tarefa 4: Insights e relat√≥rios executivos
    insights_gold_covid_19 = DatabricksRunNowOperator(
        task_id='executar_insights_gold_covid_19',
        databricks_conn_id='databricks_default',
        notebook_params={
            'data_processamento': '{{ ds }}',
            'ambiente': 'producao',
            'gerar_insights': 'true',
            'executar_consultas_analiticas': 'true',
            'criar_relatorio_executivo': 'true',
            'enviar_notificacao': 'true'
        },
        python_params=None,
        spark_submit_params=None
    )
    insights_gold_covid_19.notebook_path = "/Workspace/Users/luisgaltm@gmail.com/desafio stage/Insights_Da_Gold_covid-19"

    # Definir depend√™ncias entre as tarefas
    bronze_covid_19 >> silver_covid_19 >> gold_covid_19 >> insights_gold_covid_19

    # Documenta√ß√£o das tarefas
    bronze_covid_19.doc_md = """
    ### Bronze_covid-19 - Ingest√£o de Dados Brutos
    
    **Caminho:** `/Workspace/Users/luisgaltm@gmail.com/desafio stage/Bronze_covid-19`
    
    **Objetivo:** Ingest√£o de todas as fontes de dados COVID-19 e econ√¥micos
    **Fontes Processadas:**
    - country_dataset.csv (delimitador ;)
    - cases_dataset.csv
    - hospital_dataset.csv
    - tests_dataset.csv
    - vaccination_dataset.csv
    - API World Bank (GDP growth)
    
    **Sa√≠da:** Tabelas no schema bronze do Unity Catalog
    **Tabelas Criadas:** country_raw, cases_raw, hospital_raw, tests_raw, vaccination_raw, gdp_raw
    """

    silver_covid_19.doc_md = """
    ### Silver_covid-19 - Processamento e Qualidade
    
    **Caminho:** `/Workspace/Users/luisgaltm@gmail.com/desafio stage/Silver_covid-19`
    
    **Objetivo:** Limpeza, valida√ß√£o e enriquecimento dos dados brutos
    **Processos Executados:**
    - Convers√£o segura de tipos num√©ricos
    - Tratamento de valores missing e outliers
    - Valida√ß√£o de consist√™ncia temporal
    - Enriquecimento com joins estrat√©gicos
    - C√°lculo de m√©tricas derivadas (casos/milh√£o, etc.)
    
    **Sa√≠da:** Tabelas no schema silver do Unity Catalog  
    **Tabelas Criadas:** country_cleaned, cases_cleaned, vaccination_cleaned, gdp_cleaned, covid_enriched
    """

    gold_covid_19.doc_md = """
    ### Gold_covid-19 - Modelagem Anal√≠tica
    
    **Caminho:** `/Workspace/Users/luisgaltm@gmail.com/desafio stage/Gold_covid-19`
    
    **Objetivo:** Cria√ß√£o de modelos anal√≠ticos e agregados estrat√©gicos
    **Processos Executados:**
    - Agrega√ß√µes anuais por pa√≠s e continente
    - C√°lculo de correla√ß√µes COVID-economia
    - Cria√ß√£o de rankings de pa√≠ses
    - Desenvolvimento de √≠ndices de resili√™ncia
    - Prepara√ß√£o para dashboards executivos
    
    **Sa√≠da:** Tabelas no schema gold do Unity Catalog
    **Tabelas Criadas:** covid_economic_annual, continent_metrics, top_countries_economic_impact, top_countries_best_response
    """

    insights_gold_covid_19.doc_md = """
    ### Insights_Da_Gold_covid-19 - An√°lise e Relat√≥rios
    
    **Caminho:** `/Workspace/Users/luisgaltm@gmail.com/desafio stage/Insights_Da_Gold_covid-19`
    
    **Objetivo:** Gera√ß√£o de insights estrat√©gicos e relat√≥rios executivos
    **Processos Executados:**
    - An√°lise de correla√ß√µes entre vari√°veis
    - Identifica√ß√£o de padr√µes e tend√™ncias
    - Gera√ß√£o de relat√≥rios executivos
    - Prepara√ß√£o de dados para apresenta√ß√£o
    - C√°lculo de m√©tricas de neg√≥cio
    
    **Sa√≠das:**
    - Relat√≥rios PDF para diretoria
    - Insights para tomada de decis√£o
    - M√©tricas de performance do pipeline
    - Alertas e notifica√ß√µes
    """
```

---

## DAG Alternativa - Google Colab com API REST

### Vis√£o Geral da Integra√ß√£o Colab
Para ambientes onde o Databricks n√£o est√° dispon√≠vel, implementamos uma solu√ß√£o alternativa utilizando Google Colab como servidor de processamento com API REST.

### C√≥digo da DAG para Google Colab

```python
"""
DAG: etl_covid_colab_final
Descri√ß√£o: Pipeline ETL COVID-19 usando Google Colab como API REST
Arquitetura: Bronze ‚Üí Silver ‚Üí Gold via API autenticada
Respons√°vel: Engenharia de Dados
"""

from airflow import DAG
from airflow.providers.http.operators.http import SimpleHttpOperator
from airflow.operators.dummy import DummyOperator
from airflow.operators.python import PythonOperator
from airflow.exceptions import AirflowException
import pendulum
import json
import base64

default_args = {
    'owner': 'engenharia_dados',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': pendulum.duration(minutes=2),
    'start_date': pendulum.datetime(2025, 1, 1, tz='America/Sao_Paulo')
}

def verify_connection():
    """Verifica se a conex√£o com a API do Colab est√° configurada"""
    from airflow.hooks.base import BaseHook
    try:
        conn = BaseHook.get_connection('covid_colab_api_secure')
        print(f"‚úÖ Connection OK: {conn.conn_id}")
        return True
    except Exception as e:
        raise AirflowException(f"‚ùå Connection not found: {e}")

def validate_api_response(response):
    """Valida as respostas da API do Colab"""
    try:
        result = response.json()
        print(f"üì® Response: {result}")
        if result.get('status') in ['success', 'healthy', 'completed']:
            return result
        else:
            raise AirflowException(f"API Error: {result}")
    except ValueError as e:
        if response.status_code == 200:
            return {"status": "success", "message": "Response OK"}
        raise AirflowException(f"Invalid JSON: {response.text}")

def log_execution_results(**context):
    """Log de resultados finais da execu√ß√£o"""
    ti = context['ti']
    print("üìä ETL COVID-19 - RESULTADOS")
    
    tasks = ['executar_bronze', 'executar_silver', 'executar_gold']
    for task_id in tasks:
        try:
            result = ti.xcom_pull(task_ids=task_id)
            if result:
                status = result.get('status', 'unknown')
                message = result.get('message', 'No message')
                print(f"üìã {task_id}: {status.upper()} - {message}")
        except Exception as e:
            print(f"‚ùå {task_id}: Erro - {e}")

# Headers com autentica√ß√£o b√°sica para API do Colab
auth_string = base64.b64encode(b"covid_user:covid123").decode('utf-8')
headers = {
    "Content-Type": "application/json",
    "Authorization": f"Basic {auth_string}"
}

with DAG(
    dag_id='etl_covid_colab_final',
    default_args=default_args,
    description='Pipeline ETL COVID-19 usando Google Colab como API',
    schedule_interval=None,
    catchup=False,
    tags=['covid', 'etl', 'colab', 'api', 'rest'],
    max_active_runs=1
) as dag:

    # Task inicial
    start = DummyOperator(task_id='inicio_pipeline')

    # Verificar conex√£o
    verify_connection_task = PythonOperator(
        task_id='verificar_conexao',
        python_callable=verify_connection
    )

    # Executar camada Bronze via API
    executar_bronze = SimpleHttpOperator(
        task_id='executar_bronze_covid_19',
        http_conn_id='covid_colab_api_secure',
        endpoint='etl/bronze',
        method='POST',
        data=json.dumps({
            'data_processamento': '{{ ds }}',
            'ambiente': 'producao'
        }),
        headers=headers,
        response_check=validate_api_response,
        log_response=True,
        do_xcom_push=True
    )

    # Executar camada Silver via API
    executar_silver = SimpleHttpOperator(
        task_id='executar_silver_covid_19',
        http_conn_id='covid_colab_api_secure',
        endpoint='etl/silver',
        method='POST',
        data=json.dumps({
            'data_processamento': '{{ ds }}',
            'ambiente': 'producao'
        }),
        headers=headers,
        response_check=validate_api_response,
        log_response=True,
        do_xcom_push=True
    )

    # Executar camada Gold via API
    executar_gold = SimpleHttpOperator(
        task_id='executar_gold_covid_19',
        http_conn_id='covid_colab_api_secure',
        endpoint='etl/gold',
        method='POST',
        data=json.dumps({
            'data_processamento': '{{ ds }}',
            'ambiente': 'producao'
        }),
        headers=headers,
        response_check=validate_api_response,
        log_response=True,
        do_xcom_push=True
    )

    # Task de logging
    log_results = PythonOperator(
        task_id='log_execution_results',
        python_callable=log_execution_results,
        provide_context=True
    )

    # Task final
    end = DummyOperator(task_id='fim_pipeline')

    # Definir depend√™ncias
    start >> verify_connection_task >> executar_bronze >> executar_silver >> executar_gold >> log_results >> end
```

### Como Funciona a Integra√ß√£o com Google Colab

#### 1. **Configura√ß√£o do Ambiente Colab**
- Servidor Flask com autentica√ß√£o b√°sica
- Endpoints REST para cada camada do pipeline
- Processamento Spark distribu√≠do no Colab

#### 2. **Estrutura da API no Colab**
```python
# Endpoints dispon√≠veis
POST /etl/bronze    # Ingest√£o de dados brutos
POST /etl/silver    # Processamento e qualidade
POST /etl/gold      # Modelagem anal√≠tica
POST /etl/full      # Pipeline completo
```

#### 3. **Credenciais de Acesso**
- **URL**: `https://5000-m-s-38jjolr8rvo67-d.us-east1-2.prod.colab.dev`
- **Usu√°rio**: `covid_user`
- **Senha**: `covid123`

#### 4. **Fluxo de Execu√ß√£o**
1. Airflow envia requisi√ß√£o POST para endpoint no Colab
2. Colab executa o processamento Spark correspondente
3. Colab retorna status e m√©tricas de execu√ß√£o
4. Airflow captura resposta via XCom para logging

### Vantagens da Solu√ß√£o Colab

#### ‚úÖ **Flexibilidade**
- Execu√ß√£o sob demanda sem agendamento fixo
- F√°cil prototipagem e testes
- Ideal para ambientes de desenvolvimento

#### ‚úÖ **Custo Zero**
- Utiliza√ß√£o gratuita do Google Colab
- Sem custos de infraestrutura adicional
- Ideal para POCs e projetos acad√™micos

#### ‚úÖ **Integra√ß√£o Simplificada**
- API REST padr√£o
- Autentica√ß√£o b√°sica
- Respostas JSON estruturadas

### Configura√ß√£o Necess√°ria no Airflow

#### 1. **Connection no Airflow**
```
Connection ID: covid_colab_api_secure
Connection Type: HTTP
Host: https://5000-m-s-38jjolr8rvo67-d.us-east1-2.prod.colab.dev
Login: covid_user
Password: covid123
```

#### 2. **Pr√©-requisitos**
- Servidor Flask rodando no Colab
- Arquivos de dados carregados no ambiente Colab
- Conex√£o de internet est√°vel

---

## Compara√ß√£o das Solu√ß√µes

| Aspecto | Databricks | Google Colab |
|---------|------------|--------------|
| **Agendamento** | Autom√°tico (di√°rio) | Manual sob demanda |
| **Custo** | Corporativo | Gratuito |
| **Escalabilidade** | Alta | Limitada |
| **Confiabilidade** | Produ√ß√£o | Desenvolvimento |
| **Manuten√ß√£o** | Baixa | M√©dia |

<img width="992" height="114" alt="image" src="https://github.com/user-attachments/assets/2af1e06f-3cda-4b8b-b191-b43c8b4c41e7" />


---

## Considera√ß√µes Finais

### Para Ambiente de Produ√ß√£o
**Recomenda√ß√£o**: Utilize a DAG `etl_databricks_principal` para:
- Processamentos di√°rios automatizados
- Alta confiabilidade e escalabilidade
- Ambiente corporativo com suporte

### Para Desenvolvimento e Testes
**Recomenda√ß√£o**: Utilize a DAG `etl_covid_colab_final` para:
- Prototipagem r√°pida
- Testes de conceito
- Ambientes acad√™micos ou de estudo

Ambas as solu√ß√µes implementam a mesma arquitetura de dados (Medallion Architecture) e garantem a qualidade e consist√™ncia dos dados processados, proporcionando flexibilidade na escolha da plataforma conforme as necessidades espec√≠ficas de cada cen√°rio.



