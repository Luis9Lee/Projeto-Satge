# Camadas de Dados do COVID-19 e Economia

## Vis√£o Geral do Projeto

Este projeto implementa um pipeline completo de dados seguindo a **Medallion Architecture** no Databricks, processando dados relacionados √† COVID-19 e indicadores econ√¥micos. O pipeline √© organizado em tr√™s camadas principais (Bronze ‚Üí Silver ‚Üí Gold) com orquestra√ß√£o via Apache Airflow, proporcionando uma solu√ß√£o robusta para an√°lise do impacto da pandemia na economia global.

### Arquitetura do Sistema

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   CAMADA BRONZE ‚îÇ    ‚îÇ   CAMADA SILVER ‚îÇ    ‚îÇ    CAMADA GOLD  ‚îÇ    ‚îÇ    INSIGHTS     ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ  - Ingest√£o     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  - Limpeza      ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  - Agrega√ß√µes   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  - An√°lises     ‚îÇ
‚îÇ  - Raw Data     ‚îÇ    ‚îÇ  - Valida√ß√£o    ‚îÇ    ‚îÇ  - Modelagem    ‚îÇ    ‚îÇ  - Relat√≥rios   ‚îÇ
‚îÇ  - M√∫ltiplas    ‚îÇ    ‚îÇ  - Enriquecimento‚îÇ    ‚îÇ  - M√©tricas    ‚îÇ    ‚îÇ  - Dashboards   ‚îÇ
‚îÇ    Fontes       ‚îÇ    ‚îÇ  - Qualidade    ‚îÇ    ‚îÇ  - Rankings     ‚îÇ    ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## 1. Camada Bronze - Ingest√£o de Dados Brutos

### Objetivo
Coleta e ingest√£o inicial de dados de m√∫ltiplas fontes, preservando os dados em seu formato original para auditoria e reprocessamento.

### Fontes de Dados Processadas

| Fonte | Tipo | Descri√ß√£o | Volume Esperado |
|-------|------|-----------|-----------------|
| `country_dataset.csv` | CSV | Dados demogr√°ficos e socioecon√¥micos | ~200 pa√≠ses |
| `cases_dataset.csv` | CSV | Casos e √≥bitos por COVID-19 | ~500K registros |
| `hospital_dataset.csv` | CSV | Dados hospitalares | ~200K registros |
| `tests_dataset.csv` | CSV | Testes realizados | ~300K registros |
| `vaccination_dataset.csv` | CSV | Dados de vacina√ß√£o | ~400K registros |
| API World Bank | JSON | Crescimento do PIB (2019-2023) | ~10K registros |

### Tecnologias e T√©cnicas Utilizadas

- **Detec√ß√£o Autom√°tica de Delimitadores**: Para lidar com diferentes formatos de CSV
- **Mapeamento de Colunas**: Sistema para criar nomes de colunas v√°lidos no Delta Lake
- **Tratamento de APIs REST**: Consumo da API World Bank com tratamento de erros
- **Persist√™ncia de Metadados**: Tabelas de mapeamento para rastreabilidade

### Processos Principais

1. **Leitura Adaptativa de CSVs**: Detecta automaticamente delimitadores (`,`, `;`, `\t`, `|`)
2. **Normaliza√ß√£o de Colunas**: Cria nomes curtos e v√°lidos para o Delta Lake
3. **Ingest√£o de API**: Consome dados econ√¥micos em tempo real
4. **Controle de Qualidade**: Valida√ß√µes b√°sicas e relat√≥rios de ingest√£o

---
## C√≥digo de Configura√ß√£o Inicial (Executar Primeiro)

```python
# =============================================
# CONFIGURA√á√ÉO INICIAL DO UNITY CATALOG
# Execute este c√≥digo primeiro para configurar
# =============================================

# Configura√ß√µes para Unity Catalog
catalog_name = "`data-stage`"  # Com crases para escapar o h√≠fen
base_schema = "default"

# Criar schemas para as tr√™s camadas
spark.sql(f"CREATE SCHEMA IF NOT EXISTS {catalog_name}.bronze")
spark.sql(f"CREATE SCHEMA IF NOT EXISTS {catalog_name}.silver") 
spark.sql(f"CREATE SCHEMA IF NOT EXISTS {catalog_name}.gold")

print("Schemas criados com sucesso!")

# Listar todos os schemas no catalog
print("\n=== SCHEMAS DISPON√çVEIS ===")
schemas = spark.sql(f"SHOW SCHEMAS IN {catalog_name}")
schemas.show()
```

## C√≥digo da Camada Bronze

```python
# =============================================
# CAMADA BRONZE - INGEST√ÉO DE DADOS BRUTOS
# =============================================

from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *
import requests
import pandas as pd

# Configura√ß√µes
catalog_name = "`data-stage`"
schema_name = "bronze"

# Inicializar Spark Session
spark = SparkSession.builder.appName("Bronze_Layer").getOrCreate()

print("üöÄ INICIANDO CAMADA BRONZE")

# =============================================
# FUN√á√ïES AUXILIARES
# =============================================

def criar_nomes_colunas_seguros(df, prefixo):
    """Cria nomes de colunas curtos e v√°lidos para Delta Lake"""
    return {col: f"{prefixo}_{i:02d}" for i, col in enumerate(df.columns)}

def salvar_tabela_com_mapeamento(df, nome_tabela, prefixo_colunas):
    """Salva tabela principal e tabela de mapeamento de colunas"""
    try:
        # Criar e aplicar mapeamento de colunas
        mapeamento = criar_nomes_colunas_seguros(df, prefixo_colunas)
        for col_original, col_nova in mapeamento.items():
            df = df.withColumnRenamed(col_original, col_nova)
        
        # Salvar tabela principal
        df.write \
            .format("delta") \
            .option("mergeSchema", "true") \
            .mode("overwrite") \
            .saveAsTable(f"{catalog_name}.{schema_name}.{nome_tabela}")
        
        # Salvar tabela de mapeamento
        dados_mapeamento = [(orig, nova) for orig, nova in mapeamento.items()]
        schema_mapeamento = StructType([
            StructField("coluna_original", StringType(), True),
            StructField("coluna_curta", StringType(), True)
        ])
        df_mapeamento = spark.createDataFrame(dados_mapeamento, schema_mapeamento)
        df_mapeamento.write \
            .format("delta") \
            .mode("overwrite") \
            .saveAsTable(f"{catalog_name}.{schema_name}.{nome_tabela}_mapeamento")
        
        return True, mapeamento
        
    except Exception as e:
        print(f"‚ùå Erro ao salvar {nome_tabela}: {str(e)}")
        return False, None

def detectar_delimitador_arquivo(caminho_arquivo):
    """Detecta automaticamente o delimitador do arquivo CSV"""
    delimitadores = [",", ";", "\t", "|"]
    
    for delim in delimitadores:
        try:
            df_test = spark.read \
                .option("header", "true") \
                .option("delimiter", delim) \
                .option("inferSchema", "true") \
                .csv(caminho_arquivo)
            
            if len(df_test.columns) > 1:
                return df_test, delim
        except:
            continue
    
    return None, None

def processar_arquivo_csv(caminho_arquivo, nome_tabela, prefixo_colunas):
    """Processa arquivo CSV com delimitador autom√°tico"""
    try:
        df, delimitador = detectar_delimitador_arquivo(caminho_arquivo)
        if df is None:
            print(f"‚ùå N√£o foi poss√≠vel ler {caminho_arquivo}")
            return None
        
        print(f"‚úÖ {caminho_arquivo}: {len(df.columns)} colunas (delimitador: '{delimitador}')")
        return salvar_tabela_com_mapeamento(df, nome_tabela, prefixo_colunas)[1]
        
    except Exception as e:
        print(f"‚ùå Erro em {caminho_arquivo}: {str(e)}")
        return None

def processar_api_worldbank():
    """Ingere dados da API World Bank - GDP Growth"""
    try:
        url = "https://api.worldbank.org/v2/country/all/indicator/NY.GDP.MKTP.KD.ZG?date=2019:2023&format=json&per_page=10000"
        response = requests.get(url, timeout=30)
        data = response.json()
        
        if len(data) > 1 and isinstance(data[1], list):
            pdf = pd.json_normalize(data[1])
            colunas = ['countryiso3code', 'date', 'value', 'country.value', 'country.id', 'indicator.value']
            colunas_disponiveis = [col for col in colunas if col in pdf.columns]
            
            df_api = spark.createDataFrame(pdf[colunas_disponiveis])
            print(f"‚úÖ API World Bank: {df_api.count()} registros")
            return salvar_tabela_com_mapeamento(df_api, "gdp_raw", "gdp")[1]
        else:
            print("‚ö†Ô∏è  API n√£o retornou dados")
            return None
            
    except Exception as e:
        print(f"‚ùå Erro na API: {str(e)}")
        return None

# =============================================
# EXECU√á√ÉO PRINCIPAL
# =============================================

def executar_camada_bronze():
    """Executa toda a ingest√£o da camada Bronze"""
    
    print("üì• INICIANDO INGEST√ÉO DE DADOS...")
    mapeamentos = {}
    
    # 1. Country dataset (delimitador ;)
    print("\nüìç Country Dataset")
    df_country = spark.read \
        .option("header", "true") \
        .option("delimiter", ";") \
        .option("inferSchema", "true") \
        .csv("/Volumes/data-stage/default/dados/country_dataset.csv")
    mapeamentos["country_raw"] = salvar_tabela_com_mapeamento(df_country, "country_raw", "cou")[1]
    
    # 2. Demais datasets (detec√ß√£o autom√°tica de delimitador)
    datasets = [
        ("cases_dataset.csv", "cases_raw", "cas"),
        ("hospital_dataset.csv", "hospital_raw", "hos"), 
        ("tests_dataset.csv", "tests_raw", "tes"),
        ("vaccination_dataset.csv", "vaccination_raw", "vac")
    ]
    
    for arquivo, tabela, prefixo in datasets:
        print(f"\nüìç {arquivo}")
        mapeamento = processar_arquivo_csv(
            f"/Volumes/data-stage/default/dados/{arquivo}", tabela, prefixo)
        if mapeamento:
            mapeamentos[tabela] = mapeamento
    
    # 3. API World Bank
    print("\nüìç API World Bank")
    mapeamento_api = processar_api_worldbank()
    if mapeamento_api:
        mapeamentos["gdp_raw"] = mapeamento_api
    
    return mapeamentos

# =============================================
# RELAT√ìRIO FINAL
# =============================================

def gerar_relatorio_bronze(mapeamentos):
    """Gera relat√≥rio final da camada Bronze"""
    print("\n" + "="*50)
    print("üìä RELAT√ìRIO - CAMADA BRONZE")
    print("="*50)
    
    # Contagem de tabelas e registros
    tabelas = spark.sql(f"SHOW TABLES IN {catalog_name}.{schema_name}").collect()
    tabelas_principais = [t[1] for t in tabelas if "_mapeamento" not in t[1]]
    
    print(f"\nüìÅ Tabelas criadas: {len(tabelas_principais)}")
    
    for tabela in tabelas_principais:
        df = spark.table(f"{catalog_name}.{schema_name}.{tabela}")
        print(f"   ‚ñ∏ {tabela}: {df.count():,} registros, {len(df.columns)} colunas")
    
    print(f"\n‚úÖ CAMADA BRONZE CONCLU√çDA - {len(mapeamentos)} tabelas ingeridas")

# =============================================
# EXECU√á√ÉO
# =============================================

if __name__ == "__main__":
    mapeamentos_finais = executar_camada_bronze()
    gerar_relatorio_bronze(mapeamentos_finais)
```

## 2. Camada Silver - Limpeza e Enriquecimento

### Objetivo
Transformar dados brutos em dados confi√°veis e padronizados, aplicando regras de qualidade, limpeza e enriquecimento para prepara√ß√£o da an√°lise.

### Processos de Transforma√ß√£o

#### 2.1 Limpeza de Dados
- **Convers√£o Segura de Tipos**: Fun√ß√£o especializada para n√∫meros com diferentes formatos (v√≠rgulas, pontos)
- **Tratamento de Valores Nulos**: Filtragem de registros cr√≠ticos sem chaves
- **Padroniza√ß√£o de Datas**: Convers√£o para formato DateType do Spark
- **Valida√ß√£o de Consist√™ncia**: Filtros para popula√ß√£o > 0, datas v√°lidas

#### 2.2 Enriquecimento
- **Jun√ß√£o de Dados**: Integra√ß√£o entre casos COVID e dados demogr√°ficos
- **C√°lculo de M√©tricas**: Casos por milh√£o, √≥bitos por milh√£o
- **Extra√ß√£o de Per√≠odos**: Ano e m√™s para agrega√ß√µes temporais
- **Padroniza√ß√£o de C√≥digos**: ISO codes como chave universal

### Tabelas Produzidas

| Tabela | Descri√ß√£o | Principais Transforma√ß√µes |
|--------|-----------|---------------------------|
| `country_cleaned` | Dados demogr√°ficos limpos | Convers√£o num√©rica, filtros de qualidade |
| `cases_cleaned` | Casos e √≥bitos processados | Renomea√ß√£o, convers√£o num√©rica, padroniza√ß√£o temporal |
| `vaccination_cleaned` | Vacina√ß√£o processada | Mesmas transforma√ß√µes dos casos |
| `gdp_cleaned` | Dados econ√¥micos | Filtro temporal (2019-2023), convers√£o num√©rica |
| `covid_enriched` | Dados integrados | Joins, m√©tricas calculadas, per√≠odos extra√≠dos |

---

## C√≥digo da Camada Silver

```python
# =============================================
# CAMADA SILVER - LIMPEZA E ENRIQUECIMENTO
# =============================================

from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *

# Configura√ß√µes
catalog_name = "`data-stage`"
bronze_schema = "bronze"
silver_schema = "silver"

# Inicializar Spark Session
spark = SparkSession.builder.appName("Silver_Layer").getOrCreate()

print("üîÑ INICIANDO CAMADA SILVER")

# =============================================
# FUN√á√ïES AUXILIARES
# =============================================

def limpar_numero(coluna):
    """Converte n√∫meros com pontos/v√≠rgulas para formato double"""
    return (when(col(coluna).rlike("^[-]?[0-9]*[.,]?[0-9]+$"), 
                regexp_replace(col(coluna), "\\.", ""))
            .when(col(coluna).rlike("^[-]?[0-9]*[,]?[0-9]+$"), 
                regexp_replace(col(coluna), ",", "."))
            .otherwise(None)
            .cast("double"))

# =============================================
# PROCESSAMENTO COUNTRY
# =============================================

def processar_country():
    """Processa dados est√°ticos de pa√≠ses"""
    print("\nüìç Country Dataset")
    
    df_country = spark.table(f"{catalog_name}.{bronze_schema}.country_raw")
    
    # Aplicar transforma√ß√µes
    df_processed = (df_country
        .withColumn("population_density", limpar_numero("cou_01"))
        .withColumn("median_age", limpar_numero("cou_02"))
        .withColumn("aged_65_older", limpar_numero("cou_03"))
        .withColumn("aged_70_older", limpar_numero("cou_04"))
        .withColumn("gdp_per_capita", limpar_numero("cou_05"))
        .withColumn("extreme_poverty", limpar_numero("cou_06"))
        .withColumn("cardiovasc_death_rate", limpar_numero("cou_07"))
        .withColumn("diabetes_prevalence", limpar_numero("cou_08"))
        .withColumn("female_smokers", limpar_numero("cou_09"))
        .withColumn("male_smokers", limpar_numero("cou_10"))
        .withColumn("handwashing_facilities", limpar_numero("cou_11"))
        .withColumn("hospital_beds_per_thousand", limpar_numero("cou_12"))
        .withColumn("life_expectancy", limpar_numero("cou_13"))
        .withColumn("human_development_index", limpar_numero("cou_14"))
        .withColumn("population", col("cou_15").cast("bigint"))
        .filter(col("cou_00").isNotNull())
        .filter(col("population") > 0)
        .withColumnRenamed("cou_00", "iso_code")
        .drop(*[f"cou_{i:02d}" for i in range(16)])
    )
    
    df_processed.write \
        .format("delta") \
        .mode("overwrite") \
        .saveAsTable(f"{catalog_name}.{silver_schema}.country_cleaned")
    
    print(f"‚úÖ {df_processed.count()} pa√≠ses processados")
    return df_processed

# =============================================
# PROCESSAMENTO CASES
# =============================================

def processar_cases():
    """Processa dados de casos e √≥bitos"""
    print("\nüìç Cases Dataset")
    
    df_cases = spark.table(f"{catalog_name}.{bronze_schema}.cases_raw")
    
    # Renomear colunas
    renomear_colunas = {
        'cas_00': 'iso_code', 'cas_01': 'continent', 'cas_02': 'location',
        'cas_03': 'date', 'cas_04': 'total_cases', 'cas_05': 'new_cases',
        'cas_06': 'new_cases_smoothed', 'cas_07': 'total_deaths', 
        'cas_08': 'new_deaths', 'cas_09': 'new_deaths_smoothed',
        'cas_10': 'total_cases_per_million', 'cas_11': 'new_cases_per_million',
        'cas_12': 'new_cases_smoothed_per_million', 'cas_13': 'total_deaths_per_million',
        'cas_14': 'new_deaths_per_million', 'cas_15': 'new_deaths_smoothed_per_million'
    }
    
    df_renomeado = df_cases
    for col_curta, col_nova in renomear_colunas.items():
        if col_curta in df_renomeado.columns:
            df_renomeado = df_renomeado.withColumnRenamed(col_curta, col_nova)
    
    # Converter colunas num√©ricas
    colunas_numericas = ['total_cases', 'new_cases', 'new_cases_smoothed', 'total_deaths', 
                        'new_deaths', 'new_deaths_smoothed', 'total_cases_per_million',
                        'new_cases_per_million', 'new_cases_smoothed_per_million',
                        'total_deaths_per_million', 'new_deaths_per_million', 
                        'new_deaths_smoothed_per_million']
    
    df_processed = df_renomeado
    for coluna in colunas_numericas:
        if coluna in df_processed.columns:
            df_processed = df_processed.withColumn(coluna, limpar_numero(coluna))
    
    df_processed = (df_processed
        .withColumn("date", to_date(col("date"), "yyyy-MM-dd"))
        .filter(col("iso_code").isNotNull())
        .filter(col("date").isNotNull())
    )
    
    df_processed.write \
        .format("delta") \
        .mode("overwrite") \
        .saveAsTable(f"{catalog_name}.{silver_schema}.cases_cleaned")
    
    print(f"‚úÖ {df_processed.count()} registros processados")
    return df_processed

# =============================================
# PROCESSAMENTO VACCINATION
# =============================================

def processar_vaccination():
    """Processa dados de vacina√ß√£o"""
    print("\nüìç Vaccination Dataset")
    
    df_vaccination = spark.table(f"{catalog_name}.{bronze_schema}.vaccination_raw")
    
    # Renomear colunas
    renomear_colunas = {
        'vac_00': 'iso_code', 'vac_01': 'date', 'vac_02': 'total_vaccinations',
        'vac_03': 'people_vaccinated', 'vac_04': 'people_fully_vaccinated',
        'vac_05': 'total_boosters', 'vac_06': 'new_vaccinations',
        'vac_07': 'new_vaccinations_smoothed', 'vac_08': 'total_vaccinations_per_hundred',
        'vac_09': 'people_vaccinated_per_hundred', 'vac_10': 'people_fully_vaccinated_per_hundred',
        'vac_11': 'total_boosters_per_hundred', 'vac_12': 'new_vaccinations_smoothed_per_million',
        'vac_13': 'new_people_vaccinated_smoothed', 'vac_14': 'new_people_vaccinated_smoothed_per_hundred'
    }
    
    df_renomeado = df_vaccination
    for col_curta, col_nova in renomear_colunas.items():
        if col_curta in df_renomeado.columns:
            df_renomeado = df_renomeado.withColumnRenamed(col_curta, col_nova)
    
    # Converter colunas num√©ricas
    colunas_numericas = ['total_vaccinations', 'people_vaccinated', 'people_fully_vaccinated',
                        'total_boosters', 'new_vaccinations', 'new_vaccinations_smoothed',
                        'total_vaccinations_per_hundred', 'people_vaccinated_per_hundred',
                        'people_fully_vaccinated_per_hundred', 'total_boosters_per_hundred',
                        'new_vaccinations_smoothed_per_million', 'new_people_vaccinated_smooted',
                        'new_people_vaccinated_smoothed_per_hundred']
    
    df_processed = df_renomeado
    for coluna in colunas_numericas:
        if coluna in df_processed.columns:
            df_processed = df_processed.withColumn(coluna, limpar_numero(coluna))
    
    df_processed = (df_processed
        .withColumn("date", to_date(col("date"), "yyyy-MM-dd"))
        .filter(col("iso_code").isNotNull())
        .filter(col("date").isNotNull())
    )
    
    df_processed.write \
        .format("delta") \
        .mode("overwrite") \
        .saveAsTable(f"{catalog_name}.{silver_schema}.vaccination_cleaned")
    
    print(f"‚úÖ {df_processed.count()} registros processados")
    return df_processed

# =============================================
# ENRIQUECIMENTO DOS DADOS
# =============================================

def enriquecer_dados():
    """Cria tabela enriquecida com joins"""
    print("\nüìç Enriquecendo Dados")
    
    df_cases = spark.table(f"{catalog_name}.{silver_schema}.cases_cleaned")
    df_country = spark.table(f"{catalog_name}.{silver_schema}.country_cleaned")
    
    df_enriched = (df_cases
        .join(df_country.select("iso_code", "population", "gdp_per_capita", "life_expectancy"), 
              "iso_code", "left")
        .withColumn("cases_per_million", 
                   when(col("population") > 0, (col("total_cases") / col("population")) * 1000000)
                   .otherwise(None))
        .withColumn("deaths_per_million",
                   when(col("population") > 0, (col("total_deaths") / col("population")) * 1000000)
                   .otherwise(None))
        .withColumn("year", year(col("date")))
        .withColumn("month", month(col("date")))
    )
    
    df_enriched.write \
        .format("delta") \
        .mode("overwrite") \
        .saveAsTable(f"{catalog_name}.{silver_schema}.covid_enriched")
    
    print(f"‚úÖ {df_enriched.count()} registros enriquecidos")
    return df_enriched

# =============================================
# EXECU√á√ÉO PRINCIPAL
# =============================================

def executar_camada_silver():
    """Executa todo o processamento da camada Silver"""
    
    print("üéØ EXECUTANDO PROCESSAMENTO SILVER")
    
    # Criar schema se necess√°rio
    spark.sql(f"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{silver_schema}")
    
    # Processar datasets
    processar_country()
    processar_cases()
    processar_vaccination()
    processar_gdp()
    enriquecer_dados()
    
    print("\n‚úÖ SILVER CONCLU√çDA")

# =============================================
# RELAT√ìRIO FINAL
# =============================================

def gerar_relatorio_silver():
    """Gera relat√≥rio final da Silver"""
    print("\n" + "="*50)
    print("üìä RELAT√ìRIO - SILVER")
    print("="*50)
    
    tabelas = spark.sql(f"SHOW TABLES IN {catalog_name}.{silver_schema}").collect()
    
    for tabela in tabelas:
        nome = tabela[1]
        df = spark.table(f"{catalog_name}.{silver_schema}.{nome}")
        print(f"‚ñ∏ {nome}: {df.count():,} registros")

# =============================================
# EXECU√á√ÉO
# =============================================

if __name__ == "__main__":
    executar_camada_silver()
    gerar_relatorio_silver()
```

## 3. Camada Gold - Modelagem Anal√≠tica

### Objetivo
Criar modelos de dados otimizados para an√°lise business, com agrega√ß√µes, m√©tricas estrat√©gicas e prepara√ß√£o para visualiza√ß√£o.

### Modelos Desenvolvidos

#### 3.1 Agregados Anuais (`covid_economic_annual`)
- **Agrega√ß√µes Temporais**: Soma de casos e √≥bitos anuais
- **M√©tricas de Vacina√ß√£o**: Taxas m√°ximas anuais de vacina√ß√£o
- **Indicadores Econ√¥micos**: Crescimento do PIB integrado
- **Joins Estrat√©gicos**: Dados demogr√°ficos para contexto

#### 3.2 M√©tricas Continentais (`continent_metrics`)
- **Agrega√ß√µes Geogr√°ficas**: Somas e m√©dias por continente
- **Taxas Normalizadas**: Casos e √≥bitos por milh√£o de habitantes
- **Comparativos**: Crescimento econ√¥mico m√©dio por regi√£o

#### 3.3 Rankings Estrat√©gicos
- **Impacto Econ√¥mico**: Pa√≠ses com maior queda no PIB (2020)
- **Melhor Resposta**: Pa√≠ses com menor mortalidade (2021)
- **√çndices de Recupera√ß√£o**: Combina√ß√£o de crescimento PIB e vacina√ß√£o

#### 3.4 An√°lises de Correla√ß√£o
- **COVID vs Economia**: Correla√ß√£o entre casos e crescimento PIB
- **Vacina√ß√£o vs Recupera√ß√£o**: Impacto da vacina√ß√£o na economia
- **√çndices Compostos**: Severidade e recupera√ß√£o

### Insights Business Gerados

1. **Resili√™ncia Econ√¥mica**: Identifica√ß√£o de pa√≠ses que melhor se recuperaram
2. **Efic√°cia de Pol√≠ticas**: Correla√ß√£o entre vacina√ß√£o e performance econ√¥mica
3. **Padr√µes Regionais**: Diferen√ßas continentais na resposta √† pandemia
4. **M√©tricas de Impacto**: Severidade vs capacidade de recupera√ß√£o

---

## C√≥digo da Camada Gold

```python
# =============================================
# CAMADA GOLD - AGREGA√á√ïES E MODELAGEM FINAL
# =============================================

from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import *

# Configura√ß√µes
catalog_name = "`data-stage`"
silver_schema = "silver"
gold_schema = "gold"

# Inicializar Spark Session
spark = SparkSession.builder.appName("Gold_Layer").getOrCreate()

print("üèÜ INICIANDO CAMADA GOLD")

# =============================================
# AGREGA√á√ÉO ANUAL - COVID E ECONOMIA
# =============================================

def criar_agregados_anuais():
    """Cria tabela com agregados anuais de COVID e economia"""
    print("\nüìç Agregados Anuais")
    
    # Carregar dados da Silver
    df_covid = spark.table(f"{catalog_name}.{silver_schema}.covid_enriched")
    df_vaccination = spark.table(f"{catalog_name}.{silver_schema}.vaccination_cleaned")
    df_gdp = spark.table(f"{catalog_name}.{silver_schema}.gdp_cleaned")
    df_country = spark.table(f"{catalog_name}.{silver_schema}.country_cleaned")
    
    # Agregar casos anuais
    df_casos_anual = (df_covid
        .filter(col("year").between(2020, 2023))
        .groupBy("iso_code", "continent", "year")
        .agg(
            sum("new_cases").alias("total_cases_annual"),
            sum("new_deaths").alias("total_deaths_annual"),
            avg("cases_per_million").alias("avg_cases_per_million"),
            avg("deaths_per_million").alias("avg_deaths_per_million"),
            first("population").alias("population"),
            first("gdp_per_capita").alias("gdp_per_capita")
        )
        .filter(col("total_cases_annual").isNotNull())
    )
    
    # Agregar vacina√ß√£o anual
    df_vac_anual = (df_vaccination
        .withColumn("year", year(col("date")))
        .filter(col("year").between(2020, 2023))
        .groupBy("iso_code", "year")
        .agg(
            max("people_fully_vaccinated").alias("people_fully_vaccinated"),
            max("total_vaccinations").alias("total_vaccinations"),
            max("people_vaccinated_per_hundred").alias("vaccination_rate")
        )
    )
    
    # Join final
    df_final = (df_casos_anual
        .join(df_vac_anual, ["iso_code", "year"], "left")
        .join(df_gdp.select("iso_code", "year", "gdp_growth_percent"), 
              ["iso_code", "year"], "left")
        .join(df_country.select("iso_code", "life_expectancy", "median_age"), 
              "iso_code", "left")
        .filter(col("continent").isNotNull())
    )
    
    df_final.write \
        .format("delta") \
        .mode("overwrite") \
        .saveAsTable(f"{catalog_name}.{gold_schema}.covid_economic_annual")
    
    print(f"‚úÖ {df_final.count()} registros anuais")
    return df_final

# =============================================
# M√âTRICAS POR CONTINENTE
# =============================================

def criar_metricas_continente():
    """Cria agregados por continente"""
    print("\nüìç M√©tricas por Continente")
    
    df_anual = spark.table(f"{catalog_name}.{gold_schema}.covid_economic_annual")
    
    df_continente = (df_anual
        .groupBy("continent", "year")
        .agg(
            sum("total_cases_annual").alias("total_cases"),
            sum("total_deaths_annual").alias("total_deaths"),
            avg("gdp_growth_percent").alias("avg_gdp_growth"),
            avg("vaccination_rate").alias("avg_vaccination_rate"),
            sum("population").alias("total_population")
        )
        .withColumn("cases_per_million", (col("total_cases") / col("total_population")) * 1000000)
        .withColumn("deaths_per_million", (col("total_deaths") / col("total_population")) * 1000000)
        .filter(col("total_population") > 0)
    )
    
    df_continente.write \
        .format("delta") \
        .mode("overwrite") \
        .saveAsTable(f"{catalog_name}.{gold_schema}.continent_metrics")
    
    print(f"‚úÖ {df_continente.count()} registros continentais")
    return df_continente

# =============================================
# TOP PA√çSES - INSIGHTS
# =============================================

def criar_top_paises():
    """Cria tabelas com rankings de pa√≠ses"""
    print("\nüìç Top Pa√≠ses")
    
    df_anual = spark.table(f"{catalog_name}.{gold_schema}.covid_economic_annual")
    
    # Pa√≠ses com maior impacto econ√¥mico (queda no PIB)
    df_impacto_economico = (df_anual
        .filter(col("year") == 2020)
        .filter(col("gdp_growth_percent").isNotNull())
        .select("iso_code", "continent", "gdp_growth_percent", "total_cases_annual")
        .orderBy("gdp_growth_percent")
        .limit(20)
    )
    
    # Pa√≠ses com melhor resposta (menor mortalidade)
    df_melhor_resposta = (df_anual
        .filter(col("year") == 2021)
        .filter(col("avg_deaths_per_million").isNotNull())
        .select("iso_code", "continent", "avg_deaths_per_million", "vaccination_rate")
        .orderBy("avg_deaths_per_million")
        .limit(20)
    )
    
    df_impacto_economico.write \
        .format("delta") \
        .mode("overwrite") \
        .saveAsTable(f"{catalog_name}.{gold_schema}.top_countries_economic_impact")
    
    df_melhor_resposta.write \
        .format("delta") \
        .mode("overwrite") \
        .saveAsTable(f"{catalog_name}.{gold_schema}.top_countries_best_response")
    
    print(f"‚úÖ Rankings criados")
    return df_impacto_economico, df_melhor_resposta

# =============================================
# CORRELA√á√ÉO COVID-ECONOMIA
# =============================================

def criar_analise_correlacao():
    """Cria an√°lise de correla√ß√£o entre COVID e economia"""
    print("\nüìç An√°lise de Correla√ß√£o")
    
    df_anual = spark.table(f"{catalog_name}.{gold_schema}.covid_economic_annual")
    
    df_correlacao = (df_anual
        .filter(col("gdp_growth_percent").isNotNull())
        .filter(col("total_cases_annual").isNotNull())
        .withColumn("cases_per_capita", col("total_cases_annual") / col("population"))
        .groupBy("continent", "year")
        .agg(
            count("*").alias("country_count"),
            avg("gdp_growth_percent").alias("avg_gdp_growth"),
            avg("cases_per_capita").alias("avg_cases_per_capita"),
            avg("vaccination_rate").alias("avg_vaccination_rate"),
            corr("gdp_growth_percent", "cases_per_capita").alias("correlation_gdp_cases")
        )
        .filter(col("country_count") > 5)  # M√≠nimo de pa√≠ses para correla√ß√£o
    )
    
    df_correlacao.write \
        .format("delta") \
        .mode("overwrite") \
        .saveAsTable(f"{catalog_name}.{gold_schema}.covid_economy_correlation")
    
    print(f"‚úÖ {df_correlacao.count()} an√°lises de correla√ß√£o")
    return df_correlacao

# =============================================
# RESUMO EXECUTIVO
# =============================================

def criar_resumo_executivo():
    """Cria tabela resumo para dashboards"""
    print("\nüìç Resumo Executivo")
    
    df_continente = spark.table(f"{catalog_name}.{gold_schema}.continent_metrics")
    df_correlacao = spark.table(f"{catalog_name}.{gold_schema}.covid_economy_correlation")
    
    df_resumo = (df_continente
        .join(df_correlacao.select("continent", "year", "correlation_gdp_cases"), 
              ["continent", "year"])
        .withColumn("severity_index", col("deaths_per_million") * col("cases_per_million") / 1000)
        .withColumn("recovery_index", 
                   when(col("avg_gdp_growth") > 0, col("avg_vaccination_rate") * col("avg_gdp_growth"))
                   .otherwise(col("avg_vaccination_rate") * 0.5))
    )
    
    df_resumo.write \
        .format("delta") \
        .mode("overwrite") \
        .saveAsTable(f"{catalog_name}.{gold_schema}.executive_summary")
    
    print(f"‚úÖ Resumo executivo criado")
    return df_resumo

# =============================================
# EXECU√á√ÉO PRINCIPAL
# =============================================

def executar_camada_gold():
    """Executa todo o processamento da camada Gold"""
    
    print("üéØ EXECUTANDO PROCESSAMENTO GOLD")
    
    # Criar schema se necess√°rio
    spark.sql(f"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{gold_schema}")
    
    # Executar transforma√ß√µes
    criar_agregados_anuais()
    criar_metricas_continente()
    criar_top_paises()
    criar_analise_correlacao()
    criar_resumo_executivo()
    
    print("\n‚úÖ GOLD CONCLU√çDA")

# =============================================
# RELAT√ìRIO FINAL
# =============================================

def gerar_relatorio_gold():
    """Gera relat√≥rio final da Gold"""
    print("\n" + "="*50)
    print("üìä RELAT√ìRIO - GOLD")
    print("="*50)
    
    tabelas = spark.sql(f"SHOW TABLES IN {catalog_name}.{gold_schema}").collect()
    
    for tabela in tabelas:
        nome = tabela[1]
        df = spark.table(f"{catalog_name}.{gold_schema}.{nome}")
        print(f"‚ñ∏ {nome}: {df.count():,} registros")
        
        # Mostrar amostra para tabela principal
        if "covid_economic_annual" in nome:
            print("   üìã Amostra:")
            df.limit(3).show(truncate=False)

# =============================================
# EXECU√á√ÉO
# =============================================

if __name__ == "__main__":
    executar_camada_gold()
    gerar_relatorio_gold()
```

## C√≥digo de Insights da Gold (An√°lises e Consultas)

```python
# =============================================
# VERIFICA√á√ÉO E INSIGHTS GOLD 
# =============================================

from pyspark.sql import SparkSession
from pyspark.sql.functions import *

# Configura√ß√µes (as mesmas da Gold)
catalog_name = "`data-stage`"
gold_schema = "gold"

spark = SparkSession.builder.appName("Verificacao_Gold").getOrCreate()

def verificar_insights_gold():
    """Gera insights anal√≠ticos da camada Gold"""
    print("\nüîç INSIGHTS - CAMADA GOLD")
    
    try:
        # 1. Top pa√≠ses - impacto econ√¥mico
        print("\nüìâ TOP 10 - MAIOR QUEDA NO PIB (2020):")
        df_econ = spark.table(f"{catalog_name}.{gold_schema}.top_countries_economic_impact")
        df_econ.select("iso_code", "continent", "gdp_growth_percent").show(10, truncate=False)
        
        # 2. M√©tricas por continente
        print("\nüåç M√âTRICAS POR CONTINENTE (2021):")
        df_cont = spark.table(f"{catalog_name}.{gold_schema}.continent_metrics")
        df_cont.filter(col("year") == 2021).select(
            "continent", "total_cases", "total_deaths", "avg_gdp_growth"
        ).show(truncate=False)
        
        # 3. Dados anuais resumidos
        print("\nüìà DADOS ANUAIS - AMOSTRA:")
        df_anual = spark.table(f"{catalog_name}.{gold_schema}.covid_economic_annual")
        df_anual.filter(col("year") == 2021).select(
            "iso_code", "continent", "total_cases_annual", "gdp_growth_percent", "vaccination_rate"
        ).limit(5).show(truncate=False)
        
    except Exception as e:
        print(f"‚ùå Erro na verifica√ß√£o: {str(e)}")

# =============================================
# CONSULTAS SQL - AN√ÅLISE BUSINESS 
# =============================================

def consultas_analiticas():
    """Consultas SQL prontas para an√°lise business"""
    
    print("\nüíº CONSULTAS PARA AN√ÅLISE BUSINESS")
    
    # 1. Pa√≠ses com maior recupera√ß√£o econ√¥mica p√≥s-vacina√ß√£o
    print("\n1. Pa√≠ses com melhor recupera√ß√£o (2021):")
    spark.sql(f"""
        SELECT 
            iso_code,
            continent,
            gdp_growth_percent as gdp_2021,
            vaccination_rate,
            (gdp_growth_percent * vaccination_rate) as recovery_score
        FROM {catalog_name}.{gold_schema}.covid_economic_annual
        WHERE year = 2021 
            AND gdp_growth_percent > 0
            AND vaccination_rate > 50
        ORDER BY recovery_score DESC
        LIMIT 10
    """).show(truncate=False)
    
    # 2. Continentes com maior resili√™ncia 
    print("\n2. Resili√™ncia por continente:")
    spark.sql(f"""
        SELECT 
            continent,
            AVG(gdp_growth_percent) as avg_gdp_growth_2020_2021,
            AVG(vaccination_rate) as avg_vaccination_rate,
            AVG(total_cases_annual) as avg_casos_anuais,
            AVG(avg_casos_per_million) as avg_casos_por_milhao
        FROM {catalog_name}.{gold_schema}.covid_economic_annual
        WHERE year BETWEEN 2020 AND 2021
        GROUP BY continent
        ORDER BY avg_gdp_growth_2020_2021 DESC
    """).show(truncate=False)
    
    # 3. Correla√ß√£o entre vacina√ß√£o e crescimento econ√¥mico
    print("\n3. Impacto da vacina√ß√£o no PIB (2021):")
    spark.sql(f"""
        SELECT 
            CASE 
                WHEN vaccination_rate < 30 THEN 'Baixa Vacina√ß√£o'
                WHEN vaccination_rate BETWEEN 30 AND 60 THEN 'M√©dia Vacina√ß√£o' 
                ELSE 'Alta Vacina√ß√£o'
            END as faixa_vacinacao,
            COUNT(*) as numero_paises,
            AVG(gdp_growth_percent) as media_crescimento_pib,
            AVG(total_cases_annual) as media_casos,
            AVG(avg_casos_per_million) as media_casos_por_milhao
        FROM {catalog_name}.{gold_schema}.covid_economic_annual
        WHERE year = 2021
        GROUP BY faixa_vacinacao
        ORDER BY media_crescimento_pib DESC
    """).show(truncate=False)

# =============================================
# RELAT√ìRIO EXECUTIVO
# =============================================

def gerar_relatorio_executivo():
    """Gera relat√≥rio executivo consolidado"""
    print("\nüìä RELAT√ìRIO EXECUTIVO - IMPACTO COVID-ECONOMIA")
    print("="*60)
    
    # Resumo global
    print("\nüåé VIS√ÉO GLOBAL (2020-2021):")
    spark.sql(f"""
        SELECT 
            '2020' as ano,
            SUM(total_cases) as total_casos_globais,
            SUM(total_deaths) as total_obitos_globais,
            AVG(avg_gdp_growth) as media_crescimento_pib_global
        FROM {catalog_name}.{gold_schema}.continent_metrics
        WHERE year = 2020
        
        UNION ALL
        
        SELECT 
            '2021' as ano,
            SUM(total_cases) as total_casos_globais,
            SUM(total_deaths) as total_obitos_globais,
            AVG(avg_gdp_growth) as media_crescimento_pib_global
        FROM {catalog_name}.{gold_schema}.continent_metrics
        WHERE year = 2021
    """).show(truncate=False)
    
    # Top 5 pa√≠ses por recupera√ß√£o
    print("\nüèÜ TOP 5 PA√çSES - MELHOR RECUPERA√á√ÉO (2021):")
    spark.sql(f"""
        SELECT 
            iso_code,
            continent,
            gdp_growth_percent as crescimento_pib_2021,
            vaccination_rate as taxa_vacinacao,
            total_cases_annual as casos_2021,
            (gdp_growth_percent * vaccination_rate / 100) as indice_recuperacao
        FROM {catalog_name}.{gold_schema}.covid_economic_annual
        WHERE year = 2021 
            AND gdp_growth_percent > 0
            AND vaccination_rate > 50
        ORDER BY indice_recuperacao DESC
        LIMIT 5
    """).show(truncate=False)
    
    # Correla√ß√£o por continente
    print("\nüìà CORRELA√á√ÉO VACINA√á√ÉO x CRESCIMENTO PIB:")
    spark.sql(f"""
        SELECT 
            continent,
            year,
            avg_vaccination_rate,
            avg_gdp_growth,
            correlation_gdp_cases,
            CASE 
                WHEN correlation_gdp_cases > 0.5 THEN 'Forte Positiva'
                WHEN correlation_gdp_cases > 0.2 THEN 'Moderada Positiva'
                WHEN correlation_gdp_cases > -0.2 THEN 'Fraca'
                WHEN correlation_gdp_cases > -0.5 THEN 'Moderada Negativa'
                ELSE 'Forte Negativa'
            END as intensidade_correlacao
        FROM {catalog_name}.{gold_schema}.covid_economy_correlation
        WHERE year IN (2020, 2021)
        ORDER BY continent, year
    """).show(truncate=False)

# =============================================
# EXECU√á√ÉO PRINCIPAL
# =============================================

def executar_insights_gold():
    """Executa todos os insights da camada Gold"""
    
    print("üéØ EXECUTANDO AN√ÅLISES E INSIGHTS")
    
    # Verifica√ß√£o b√°sica dos dados
    verificar_insights_gold()
    
    # Consultas anal√≠ticas
    consultas_analiticas()
    
    # Relat√≥rio executivo
    gerar_relatorio_executivo()
    
    print("\n‚úÖ INSIGHTS CONCLU√çDOS - DADOS PRONTOS PARA DECIS√ÉO")

# =============================================
# EXECU√á√ÉO
# =============================================

if __name__ == "__main__":
    executar_insights_gold()
```

## Resumo Completo da Sequ√™ncia de Execu√ß√£o:

1. **Primeiro**: Executar o c√≥digo de **Configura√ß√£o Inicial**
2. **Segundo**: Executar a **Camada Bronze** 
3. **Terceiro**: Executar a **Camada Silver**
4. **Quarto**: Executar a **Camada Gold**
5. **Quinto**: Executar os **Insights da Gold**

Cada c√≥digo √© independente e deve ser executado nessa ordem para garantir que o pipeline funcione corretamente. Os insights da Gold s√£o opcionais e servem para gerar an√°lises business e relat√≥rios executivos com base nos dados j√° processados.
